{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Контекстно-независимое векторное представление слов \n",
    "\n",
    "1. Используйте тексты из предыдущего задания и обучите на них модель векторного представления слов (опционально можно приводить слова к нормальной форме и удалить стоп-слова). Можно использовать `gensim`.\n",
    "\n",
    "2. Разделите коллекцию текстов на обучающее и тестовое множество. С помощью обученной модели векторного представления отобразите каждый документ в вектор, усреднив все вектора для слов документа. \n",
    "\n",
    "3. Используйте какой-либо алгоритм классификации (например `SVM`) для классификации текстов. Для обучения используйте тестовое множество, для анализа результатов - тестовое.\n",
    "\n",
    "4. Простое усреднение векторов слов обычно не дает хорошего отображения документа в вектор. Придумайте альтернативный способ. Протестируйте его, повторно обучив алгоритм классификации на тех же данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alyonka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17487025 -0.21993937  0.23280168 -0.9565174  -0.46915236 -0.5328271\n",
      "  0.5924817   0.7911074  -0.53437495 -0.14861862  0.37876156 -1.1040527\n",
      " -0.6631277   0.22350211  0.2631207  -0.10909087  0.21100278  0.13514163\n",
      " -0.05833348 -1.7306471   0.40038756 -0.09758746  0.35301742  0.5998624\n",
      " -0.50055236  0.96901643 -0.18253003 -0.8461332  -0.16839561  0.58657\n",
      "  0.22522123  0.26704293  0.35815004  0.0048836   1.0009377  -0.30500847\n",
      "  0.5190053  -0.36052087 -1.0223254  -0.46223456  0.50594187 -0.68825585\n",
      " -0.4451541   0.2518149   0.9818086  -0.74031323 -0.8163607   0.01008113\n",
      " -0.5282565   0.28519088 -0.3386427  -0.3969391  -0.21881814 -0.33663973\n",
      " -0.61325014 -0.41186818 -0.3136901  -0.25263292  0.01260973  0.32684645\n",
      "  0.00834684 -0.31776896  0.11361655  0.3674609  -0.86514896  0.7830459\n",
      " -0.15950629  0.7379268   0.2604584   0.30316347  0.10879931  0.6557162\n",
      "  0.6571732  -0.05581065  0.09422336  0.55482006  0.56319994 -0.05069923\n",
      " -0.41331205 -0.17503008 -0.679263    0.3421092   0.12921241  0.79133576\n",
      " -0.5652197  -0.32081687  0.26840183  0.3281375   0.48845658  0.00597967\n",
      "  0.7059681  -0.00441052  0.954307    0.9196676   1.0661348   0.37370765\n",
      "  1.1807944  -0.38701537  0.13444814  0.58184695]\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "  \n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import gzip\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    stop_words = set(stopwords.words('russian'))  \n",
    "    text_str = ' '.join(text)\n",
    "    return [word for word in simple_preprocess(text_str) if word not in stop_words]\n",
    "\n",
    "def load_texts_from_gzip(file_path):\n",
    "    texts = []\n",
    "    categories = []\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as gz_file:\n",
    "        for line in gz_file:\n",
    "            category, title, text = line.strip().split('\\t')\n",
    "            full_text = f\"{title} {text}\"             \n",
    "            categories.append(category)\n",
    "            texts.append(simple_preprocess(full_text, deacc=True))  \n",
    "    return texts, categories\n",
    "\n",
    "file_path = \"D://Nlp'23//nlp-2023//data//news.txt.gz\"\n",
    "texts, categories = load_texts_from_gzip(file_path)\n",
    "\n",
    "\n",
    "processed_texts = [preprocess(text) for text in texts]\n",
    "\n",
    "\n",
    "model = Word2Vec(processed_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.save('word2vec_model.bin')\n",
    "\n",
    "vector_science = model.wv['science']\n",
    "print(vector_science)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.8666385e-02  4.9478608e-01  2.6955810e-01  6.8637542e-02\n",
      " -1.4511509e-01 -7.1756470e-01  4.6641585e-01  1.0526983e+00\n",
      " -3.7028247e-01 -6.2013716e-01 -1.9599545e-01 -9.0326405e-01\n",
      " -2.1689889e-01  3.7545747e-01  1.6277935e-01 -4.3027753e-01\n",
      "  2.5271619e-02 -3.6724231e-01  6.7544994e-03 -1.1860762e+00\n",
      "  2.2877713e-01 -8.5457589e-04  4.5040783e-01 -3.3930022e-02\n",
      " -1.9753447e-01  2.3945320e-01 -3.6652562e-01 -2.5676030e-01\n",
      " -3.9200771e-01  3.9809299e-01  4.9960732e-01  2.3476968e-02\n",
      "  3.6575682e-02 -4.7432020e-01 -6.2942944e-02  3.0887750e-01\n",
      "  3.0142668e-01 -2.4140991e-01 -5.3324974e-01 -7.4597782e-01\n",
      "  3.9856958e-01 -5.2596545e-01 -3.3850017e-01  1.5693341e-01\n",
      "  6.0162336e-01 -3.3280191e-01 -4.6111122e-01  2.2933837e-02\n",
      " -1.1193115e-01  3.9802951e-01  3.2288313e-01 -6.0498595e-01\n",
      " -3.3605403e-01 -3.3113295e-01 -6.9185495e-01  1.4733934e-01\n",
      "  2.0853600e-01 -1.7927408e-01 -2.9223657e-01  6.5758258e-02\n",
      "  1.3364650e-01  1.2357526e-02  3.4264911e-02  1.8912803e-02\n",
      " -4.9183595e-01  6.9017375e-01  1.3427795e-01  5.2065355e-01\n",
      " -6.0052377e-01  4.9358338e-01 -2.3331946e-01  5.6960416e-01\n",
      "  5.9377402e-01 -1.8343951e-01  3.5925722e-01  4.0921053e-01\n",
      "  1.3790224e-02  1.6671424e-01 -3.9129815e-01  2.5225738e-01\n",
      " -5.3784692e-01 -2.0887828e-01 -2.1459804e-01  3.7298822e-01\n",
      " -1.6261975e-01 -2.0428963e-01  2.5770384e-01  3.5690084e-01\n",
      "  6.1066622e-01  2.9839417e-01  5.9715289e-01  1.9901608e-01\n",
      "  3.0879104e-01  1.8676837e-01  9.8116195e-01  1.8560562e-01\n",
      "  5.2160299e-01 -5.1011294e-01  2.2934411e-01  1.7399219e-01]\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "texts_train, texts_test, categories_train, categories_test = train_test_split(processed_texts, categories, test_size=0.2, random_state=42)\n",
    "\n",
    "def document_vector(model, doc):\n",
    "    return np.mean([model.wv[word] for word in doc if word in model.wv], axis=0)\n",
    "\n",
    "train_vectors = [document_vector(model, doc) for doc in texts_train]\n",
    "test_vectors = [document_vector(model, doc) for doc in texts_test]\n",
    "\n",
    "print(train_vectors[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.696\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.50      0.01      0.02        79\n",
      "     culture       0.67      0.67      0.67       279\n",
      "   economics       0.65      0.88      0.75       266\n",
      "      forces       0.56      0.65      0.60       149\n",
      "        life       0.64      0.69      0.67       288\n",
      "       media       0.71      0.68      0.69       299\n",
      "     science       0.69      0.69      0.69       288\n",
      "       sport       0.91      0.92      0.92       276\n",
      "       style       0.86      0.32      0.46        38\n",
      "      travel       0.29      0.05      0.09        38\n",
      "\n",
      "    accuracy                           0.70      2000\n",
      "   macro avg       0.65      0.56      0.56      2000\n",
      "weighted avg       0.69      0.70      0.68      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3 \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(train_vectors, categories_train)\n",
    "\n",
    "predictions = svm_classifier.predict(test_vectors)\n",
    "\n",
    "accuracy = accuracy_score(categories_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(categories_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо простого усреднения векторов слов, можно использовать взвешенное усреднение, учитывая важность каждого слова. Можно, например, использовать веса, основанные на обратной частоте документов (TF-IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (TF-IDF): 0.869\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.90      0.33      0.48        79\n",
      "     culture       0.90      0.91      0.91       279\n",
      "   economics       0.78      0.95      0.86       266\n",
      "      forces       0.82      0.85      0.83       149\n",
      "        life       0.81      0.90      0.85       288\n",
      "       media       0.90      0.81      0.85       299\n",
      "     science       0.87      0.89      0.88       288\n",
      "       sport       0.97      0.98      0.98       276\n",
      "       style       0.96      0.71      0.82        38\n",
      "      travel       0.92      0.61      0.73        38\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.88      0.79      0.82      2000\n",
      "weighted avg       0.87      0.87      0.86      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "train_tfidf = tfidf_vectorizer.fit_transform([' '.join(doc) for doc in texts_train])\n",
    "test_tfidf = tfidf_vectorizer.transform([' '.join(doc) for doc in texts_test])\n",
    "\n",
    "svm_classifier_tfidf = SVC(kernel='linear')\n",
    "svm_classifier_tfidf.fit(train_tfidf, categories_train)\n",
    "\n",
    "predictions_tfidf = svm_classifier_tfidf.predict(test_tfidf)\n",
    "\n",
    "accuracy_tfidf = accuracy_score(categories_test, predictions_tfidf)\n",
    "print(f'Accuracy (TF-IDF): {accuracy_tfidf}')\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(categories_test, predictions_tfidf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметно улучшение в сравнении с предыдущим подходом"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
